{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d16baef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afdfd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version\n",
    "def norm_corr(x,y):\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return [0,0]\n",
    "    N = x.size + y.size - 1\n",
    "    # normalize x and y respectively \n",
    "    normx = np.linalg.norm(x)\n",
    "    x1 = x/normx\n",
    "    normy = np.linalg.norm(y)\n",
    "    y1 = y/normy\n",
    "    # pad zeros in front to make the length x+y-1\n",
    "    x1=np.concatenate([x1, np.zeros(N-x.size)])\n",
    "    y1 = np.concatenate([y1, np.zeros(N-y.size)])\n",
    "    #x1 = np.pad(x1, (N-1-x.size, 0), 'constant', constant_values=(0, 0))\n",
    "    #y1 = np.pad(y1, (N-1-y.size, 0), 'constant', constant_values=(0, 0))\n",
    "    xfft = np.fft.fft(x1)\n",
    "    yfft = np.fft.fft(y1)\n",
    "    corr = np.fft.ifft(np.multiply(np.conjugate(xfft), yfft))\n",
    "\n",
    "    return corr\n",
    "\n",
    "def autocorrelate(y, sr):\n",
    "    # Perform autocorrelation on y\n",
    "    ncorr = norm_corr(y,y)\n",
    "    N = y.size\n",
    "\n",
    "    # Show only the left half (positive x) to compare with spectrogram \n",
    "    corr_pos = ncorr[0:N]\n",
    "\n",
    "    t = np.arange(0, N/sr, 1/sr)\n",
    "\n",
    "#     # Figure size can be changed here\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "\n",
    "#     # Show all integer time values\n",
    "#     plt.xticks(np.arange(min(t), max(t)+1, 5.0))\n",
    "#     plt.plot(t, corr_pos)\n",
    "#     #plt.ylim(bottom = 0, top=0.2)\n",
    "    \n",
    "    # Discard the first 2 seconds \n",
    "    new_corr_pos = corr_pos[2*sr:]\n",
    "    new_t = np.arange(2, len(new_corr_pos)/sr+2, 1/sr)\n",
    "    \n",
    "#     plt.xticks(np.arange(min(new_t), max(new_t)+1, 8.0))\n",
    "#     plt.plot(new_t, new_corr_pos)\n",
    "#     plt.ylabel(\"corr\")\n",
    "#     plt.xlabel('Time of shift')\n",
    "#     plt.title(\"Auto-correlation\")\n",
    "\n",
    "    # Find peaks in autocorrelation result\n",
    "    # using signal package \n",
    "    distance = sr\n",
    "    peaks, properties = signal.find_peaks(new_corr_pos, distance=distance, height=0)\n",
    "    heights = properties['peak_heights']\n",
    "    max_height = heights.max()\n",
    "    h_thres = max_height * 0.6\n",
    "\n",
    "    peaks, properties = signal.find_peaks(new_corr_pos, distance=distance, height=h_thres)\n",
    "    heights = properties['peak_heights']\n",
    "    peaks_locs = 2+peaks/sr\n",
    "    \n",
    "    print(\"Peak locations (in seconds):\", peaks_locs)\n",
    "    \n",
    "    # plot peaks \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.title('Peak picking (distance=%2.0f samples, Fs=%3.0f, height >%0.3f)'%(distance, sr,h_thres))\n",
    "    plt.ylabel(\"corr\")\n",
    "    plt.xlabel('Time of shift (s)')\n",
    "    plt.xticks(np.arange(min(new_t), max(new_t)+1, 4.0))\n",
    "    plt.plot(new_t, new_corr_pos)\n",
    "    plt.vlines(peaks_locs, 0, 1, color='r', linestyle=':', linewidth=1);\n",
    "    return peaks_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c1498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances_in_measures(distances_s, idx, analysis_dir):\n",
    "    # convert distances from seconds to beats\n",
    "    tempo_txt = analysis_dir+idx+'/tempo.txt'\n",
    "    with open(tempo_txt, 'r') as file:\n",
    "        # Read tempo of the song\n",
    "        tempo = int(file.readline())  # beats per minute\n",
    "        \n",
    "    song_db = pd.read_csv(analysis_dir+'index.csv')\n",
    "    song_meta = song_db[song_db['song_id'] == int(idx.lstrip('0'))]\n",
    "    beats_per_bar = song_meta['num_beats_per_measure'].values[0] \n",
    "    beats_per_measure = beats_per_bar*2\n",
    "\n",
    "    beat_duration = 60/tempo # duration of one beat\n",
    "    distances_in_beats = distances_s/beat_duration\n",
    "    distances_in_measures = distances_in_beats/beats_per_measure\n",
    "    \n",
    "    return distances_in_measures, beats_per_measure \n",
    "\n",
    "def get_beats_per_measure(idx, analysis_dir):\n",
    "    # convert distances from seconds to beats\n",
    "    tempo_txt = analysis_dir+idx+'/tempo.txt'\n",
    "    with open(tempo_txt, 'r') as file:\n",
    "        # Read tempo of the song\n",
    "        tempo = int(file.readline())  # beats per minute\n",
    "        \n",
    "    song_db = pd.read_csv(analysis_dir+'index.csv')\n",
    "    song_meta = song_db[song_db['song_id'] == int(idx.lstrip('0'))]\n",
    "    beats_per_measure = song_meta['num_beats_per_measure'].values[0] \n",
    "    \n",
    "    return beats_per_measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836534c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "\n",
    "def affinity_naive(data, threshold):\n",
    "    ln = len(data)\n",
    "    A = [[0 for j in range(ln)] for i in range(ln)]\n",
    "    for i in range(ln):\n",
    "        for j in range(ln):\n",
    "            if i>j: # matrix is symmetric\n",
    "                A[i][j] = A[j][i]\n",
    "            else:\n",
    "                corr = np.max(norm_corr(data[i],data[j])[:1000])\n",
    "                A[i][j] = corr\n",
    "    return A\n",
    "\n",
    "def plot_diagnoal(data, i, j, dur):\n",
    "    for k in range(dur+1):\n",
    "        data[i+k][j+k] = 1\n",
    "                            \n",
    "def affinity_1(data, threshold, shortest,longest):\n",
    "    ln = len(data)\n",
    "    A = [[0 for j in range(ln)] for i in range(ln)]\n",
    "    i = 0\n",
    "    while i < ln:\n",
    "        # j > i\n",
    "        j = i+1\n",
    "        matching_pairs = []\n",
    "        while j < ln:\n",
    "            print(i,j)\n",
    "            corr = norm_corr(data[i],data[j])[0]\n",
    "            A[i][j] = corr\n",
    "            if corr > threshold:\n",
    "                # stretch that segment diagonally \n",
    "                print('stretch')\n",
    "                curr_len = 0\n",
    "                curr_i = i\n",
    "                curr_j = j\n",
    "                while curr_len < longest:\n",
    "                    print('curr len:', curr_len)\n",
    "                    if curr_i+1 == ln or curr_j+1 == ln:\n",
    "                        break\n",
    "                    else:\n",
    "                        curr_i += 1\n",
    "                        curr_j += 1\n",
    "                        next_in_diag = norm_corr(data[curr_i],data[curr_j])[0]\n",
    "                        A[curr_i][curr_j] = next_in_diag\n",
    "                        if next_in_diag > threshold:\n",
    "                            curr_len += 1\n",
    "                        else:\n",
    "                            break\n",
    "                if curr_len > shortest:\n",
    "                    matching_pairs.append([i,j,curr_len])\n",
    "                    j = curr_j\n",
    "                else:\n",
    "                    j+=1\n",
    "            else:\n",
    "                j += 1\n",
    "        # iterate matching pairs to plot for other pairs \n",
    "        for k in range(len(matching_pairs)):\n",
    "            for p in range(k+1,len(matching_pairs)):\n",
    "                new_i = matching_pairs[k][1]\n",
    "                new_j = matching_pairs[p][1]\n",
    "                dur = min(matching_pairs[k][2], matching_pairs[p][2])\n",
    "                # new_i < new_j\n",
    "                plot_diagnoal(A,new_i,new_j,dur)\n",
    "                \n",
    "        if len(matching_pairs)>0:\n",
    "            print(matching_pairs)\n",
    "            i+=matching_pairs[0][2]\n",
    "        else:\n",
    "            i+= 1\n",
    "    \n",
    "    return A\n",
    "            \n",
    "    \n",
    "\n",
    "def affinity_heuristic(data, distances, shortest,longest):\n",
    "    ln = len(data)\n",
    "    thresh = 0.5\n",
    "    A = [[0 for j in range(ln)] for i in range(ln)]\n",
    "\n",
    "    # Initialize matrix, compute A[i][i+d] for all d in distances\n",
    "    for i in range(ln):\n",
    "        for d in distances:\n",
    "            if i+d < ln:\n",
    "                A[i][i+d] = np.max(norm_corr(data[i],data[i+d])[:1000])\n",
    "                \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            if i>j: # matrix is symmetric\n",
    "                A[i][j] = A[j][i]\n",
    "            if A[i][j]>thresh:\n",
    "                # stretch that segment diagonally \n",
    "                curr_len = 1\n",
    "                curr_i = i\n",
    "                curr_j = j\n",
    "                while curr_len < longest:\n",
    "                    if curr_i+1 == ln or curr_j+1 == ln:\n",
    "                        break\n",
    "                    else:\n",
    "                        curr_i += 1\n",
    "                        curr_j += 1\n",
    "                        next_in_diag = np.max(norm_corr(data[curr_i],data[curr_j])[:1000])\n",
    "                        A[curr_i][curr_j] = next_in_diag\n",
    "                        if next_in_diag > thresh:\n",
    "                            curr_len += 1\n",
    "                        else:\n",
    "                            break\n",
    "    return A\n",
    "\n",
    "# Diagonal smoothing\n",
    "def smooth(A):\n",
    "    smoothed = [[0 for j in range(len(A[0]))] for i in range(len(A))]\n",
    "\n",
    "\n",
    "    for i in range(len(A)-1):\n",
    "        for j in range(len(A[0])-1):\n",
    "            if i > 0 and j > 0:\n",
    "                smoothed[i][j] = statistics.median([A[i-1][j-1],A[i][j], A[i+1][j+1]])\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69231b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beat_segments(y, sr, idx):\n",
    "    beat_midi = f'../music_structure_analysis/POP909-Dataset-midi/POP909/{idx}/beat_midi.txt'\n",
    "    chord_midi = f'../music_structure_analysis/POP909-Dataset-midi/POP909/{idx}/chord_midi.txt'\n",
    "    df_beat = pd.read_csv(beat_midi, header = None, sep =' ', names=['time','b1','b2'])\n",
    "    df_chord = pd.read_csv(chord_midi, header = None, sep = '\\t', names=['start','end','chord'])\n",
    "    chords = df_chord['chord']\n",
    "    k = 0 \n",
    "    found = False\n",
    "    while not found:\n",
    "        chord = chords[k]\n",
    "        if chord != 'N':\n",
    "            found = True\n",
    "            break\n",
    "        k += 1\n",
    "\n",
    "    df_beat = df_beat.loc[k:].reset_index(drop=True)\n",
    "    beat_times = df_beat.iloc[:,0]\n",
    "    \n",
    "    # Extract audio segments corresponding to each beat\n",
    "    beat_segments = []\n",
    "\n",
    "    for i in range(len(beat_times)-1):\n",
    "        # Calculate sample index corresponding to current beat\n",
    "        sample_index_start = librosa.time_to_samples(beat_times[i], sr=sr)\n",
    "        sample_index_end = librosa.time_to_samples(beat_times[i+1], sr=sr)\n",
    "        # Extract audio segment corresponding to current beat\n",
    "        beat_segments.append(y[sample_index_start: sample_index_end])\n",
    "    \n",
    "    return beat_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe7898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_beats_to_measures(beat_segments, beats_per_measure):\n",
    "    # Create a new list to store the merged segments (merge 3/4 beats together)\n",
    "    merged_segments = []\n",
    "    if beats_per_measure == 2:\n",
    "        beats_per_measure = 4\n",
    "    # Loop over the segments and merge adjacent groups of four segments\n",
    "    for i in range(0, len(beat_segments), beats_per_measure):\n",
    "        # Create a new merged segment by concatenating four adjacent segments\n",
    "        merged_segment = beat_segments[i]\n",
    "        for j in range(1, beats_per_measure):\n",
    "            if i+j < len(beat_segments):\n",
    "                merged_segment = np.concatenate((merged_segment, beat_segments[i+j]))\n",
    "\n",
    "        # Add the new merged segment to the list of merged segments\n",
    "        merged_segments.append(merged_segment)\n",
    "    \n",
    "    return merged_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62805a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(idx, analysis_dir, midi_dir):\n",
    "    \n",
    "    # Load audio and samping rate \n",
    "    print('Loading audio...')\n",
    "    filename = midi_dir+idx+'/'+'melody_extracted.wav'\n",
    "    print(filename)\n",
    "    y, sr = librosa.load(filename, sr=None)\n",
    "    \n",
    "    # Perform autocorrelation on y to get distances between repetitions\n",
    "    print('Performing autocorrelation...')\n",
    "    distances_s = autocorrelate(y,sr)\n",
    "    \n",
    "    # Convert distances from seconds to quarters\n",
    "    # so as to align with human-labels provided\n",
    "    distances_measures, beats_per_measure = get_distances_in_measures(distances_s, idx, analysis_dir)\n",
    "    distances_measures = [round(x) for x in distances_measures if x >= 4]\n",
    "    print(distances_measures)\n",
    "    \n",
    "    # Sync audio with beats\n",
    "    # Read beat times from CSV file\n",
    "    beat_file = midi_dir+idx+'/beat_audio.txt'\n",
    "    beat_segments = get_beat_segments(y, sr, idx)\n",
    "    \n",
    "    # Merge beats into bars\n",
    "    print('Merging beats to measures...')\n",
    "    measure_segments = merge_beats_to_measures(beat_segments, beats_per_measure)\n",
    "    \n",
    "    # Construct recurrence matrix \n",
    "    min_num_of_measures = 4\n",
    "    max_num_of_measures = 20 # 20 measures at most\n",
    "    A = affinity_naive(bar_segments, distances_measures, min_num_of_measures, max_num_of_bars)\n",
    "#     A_abs = [[abs(A[i][j]) for j in range(len(A[0]))] for i in range(len(A))]\n",
    "#     A_filt = [[1 if A_abs[i][j] > 0.5 and i != j else 0 for j in range(len(A_abs[0]))] for i in range(len(A_abs))]\n",
    "\n",
    "\n",
    "    # Plot recurrence matrix \n",
    "    print('Plotting recurrence matrix...')\n",
    "    plt.imshow(A)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return A\n",
    "    \n",
    "#     # Apply spectral clustering to affinity matrix\n",
    "#     print('Clustering...')\n",
    "#     from sklearn.cluster import SpectralClustering\n",
    "#     n_clusters = 4  # number of clusters to group the audio into\n",
    "#     clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed').fit(A_filt)\n",
    "\n",
    "#     # Find the boundaries between clusters\n",
    "#     cluster_boundaries = [0] + list(np.where(np.diff(clustering.labels_))[0] + 1) + [len(clustering.labels_)]\n",
    "#     print(cluster_boundaries)\n",
    "#     # Print the boundaries of each cluster\n",
    "#     for i in range(len(cluster_boundaries) - 1):\n",
    "#         print(f'Cluster {i + 1}: {cluster_boundaries[i]}-{cluster_boundaries[i+1]-1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bbc6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def store_recurrence_matrix(idx,analysis_dir,midi_dir):\n",
    "    \n",
    "    # Load audio and samping rate \n",
    "    print('Loading audio...')\n",
    "    filename = midi_dir+idx+'/'+idx+'.wav'\n",
    "    print(filename)\n",
    "    y, sr = librosa.load(filename, sr=None)\n",
    "    \n",
    "#     # Perform autocorrelation on y to get distances between repetitions\n",
    "#     print('Performing autocorrelation...')\n",
    "#     distances_s = autocorrelate(y,sr)\n",
    "#     distances_s= [0,1]\n",
    "    \n",
    "    # Convert distances from seconds to quarters\n",
    "    # so as to align with human-labels provided\n",
    "    beats_per_measure = get_beats_per_measure(idx, analysis_dir)\n",
    "#     distances_measures = [round(x) for x in distances_measures if x >= 4]\n",
    "    \n",
    "    # Sync audio with beats\n",
    "    # Read beat times from CSV file\n",
    "    beat_file = midi_dir+idx+'/beat_audio.txt'\n",
    "    beat_segments = get_beat_segments(y, sr, idx)\n",
    "    \n",
    "    # Merge beats into bars\n",
    "    print('Merging beats to measures...')\n",
    "    measure_segments = merge_beats_to_measures(beat_segments, beats_per_measure)\n",
    "    \n",
    "    # Construct recurrence matrix \n",
    "#     min_num_of_measures = 4\n",
    "#     max_num_of_measures = 20 # 20 measures at most\n",
    "    A = affinity_naive(measure_segments,threshold=0)\n",
    "    A_abs = [[abs(A[i][j]) for j in range(len(A[0]))] for i in range(len(A))]\n",
    "    \n",
    "    with open(f'{idx}_RM.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # write the data to the CSV file\n",
    "        for row in A_abs:\n",
    "            writer.writerow(row)\n",
    "#     A_filt = [[1 if A_abs[i][j] > 0.5 and i != j else 0 for j in range(len(A_abs[0]))] for i in range(len(A_abs))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3482695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def get_annotation(path):\n",
    "    '''\n",
    "    Reads structure and converts start and end times to symbolic ticks. Each tick is a 16th note.    \n",
    "    '''\n",
    "    string = str(np.loadtxt(path, dtype='str'))\n",
    "    # use regular expressions to separate the alphabet and number characters\n",
    "    alpha_list = re.findall('[a-zA-Z]', string)\n",
    "    num_list = [int(s) for s in re.findall(r'\\d+', string)]\n",
    "\n",
    "    # create a dictionary with the separated lists\n",
    "    data = {'phrase': alpha_list, 'num_bars': num_list}\n",
    "    \n",
    "    structure = pd.DataFrame(data)\n",
    "\n",
    "    # Remove lowercase phrase labels, in Dai et. al 2021, they are non-melodic phrases, and don't repeat in music.\n",
    "    structure['num_bars'] = pd.to_numeric(structure['num_bars'])\n",
    "    total_num_of_bars = sum(structure['num_bars'])\n",
    "    structure['duration'] = structure['num_bars']  # duration = 1 measure (actually 2 bars)\n",
    "    structure['end'] = structure['duration'].cumsum()\n",
    "    structure['start'] = structure['end'] - structure['duration']\n",
    "#     structure = structure[structure['phrase'].str.match(r'(^[A-Z].*)')==True]\n",
    "\n",
    "    return structure, total_num_of_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eee0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recurrence_matrix(label):\n",
    "    structure, total_num_of_bars = get_annotation(label)\n",
    "    # modify number of bars -> * 2 \n",
    "    \n",
    "    A =  [[0 for j in range(total_num_of_bars)] for i in range(total_num_of_bars)]\n",
    "    groups = structure.groupby('phrase').groups\n",
    "    print(structure)\n",
    "    \n",
    "\n",
    "    for label in groups:\n",
    "        if label.isupper() and label != 'X':\n",
    "            seg_idx = groups[label]\n",
    "            for i in seg_idx:\n",
    "                for j in seg_idx:\n",
    "                    if i != j:\n",
    "#                         print(label,i,j)\n",
    "                        start_i = structure.iloc[i]['start']\n",
    "                        start_j = structure.iloc[j]['start']\n",
    "                        dur = structure.iloc[i]['duration']\n",
    "                        while dur != 0 and start_i :\n",
    "                            A[start_i][start_j] = 1\n",
    "                            start_i+=1 \n",
    "                            start_j+=1\n",
    "                            dur -= 1\n",
    "    plt.imshow(A)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6161f1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio...\n",
      "../music_structure_analysis/POP909-Dataset-midi/POP909/001/001.wav\n",
      "Merging beats to measures...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# get the index string for filename\u001b[39;00m\n\u001b[1;32m      8\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:03d}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mstore_recurrence_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmidi_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmidi_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalysis_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalysis_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mstore_recurrence_matrix\u001b[0;34m(idx, analysis_dir, midi_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m     measure_segments \u001b[38;5;241m=\u001b[39m merge_beats_to_measures(beat_segments, beats_per_measure)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Construct recurrence matrix \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     min_num_of_measures = 4\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#     max_num_of_measures = 20 # 20 measures at most\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43maffinity_naive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasure_segments\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     A_abs \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mabs\u001b[39m(A[i][j]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(A[\u001b[38;5;241m0\u001b[39m]))] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(A))]\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_RM.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36maffinity_naive\u001b[0;34m(data, threshold)\u001b[0m\n\u001b[1;32m     10\u001b[0m             A[i][j] \u001b[38;5;241m=\u001b[39m A[j][i]\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m             corr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[43mnorm_corr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m1000\u001b[39m])\n\u001b[1;32m     13\u001b[0m             A[i][j] \u001b[38;5;241m=\u001b[39m corr\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mnorm_corr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#x1 = np.pad(x1, (N-1-x.size, 0), 'constant', constant_values=(0, 0))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#y1 = np.pad(y1, (N-1-y.size, 0), 'constant', constant_values=(0, 0))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m xfft \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft(x1)\n\u001b[0;32m---> 17\u001b[0m yfft \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfft\u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m corr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mifft(np\u001b[38;5;241m.\u001b[39mmultiply(np\u001b[38;5;241m.\u001b[39mconjugate(xfft), yfft))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m corr\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mfft\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/fft/_pocketfft.py:215\u001b[0m, in \u001b[0;36mfft\u001b[0;34m(a, n, axis, norm)\u001b[0m\n\u001b[1;32m    213\u001b[0m     n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m    214\u001b[0m inv_norm \u001b[38;5;241m=\u001b[39m _get_forward_norm(n, norm)\n\u001b[0;32m--> 215\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_fft\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/fft/_pocketfft.py:70\u001b[0m, in \u001b[0;36m_raw_fft\u001b[0;34m(a, n, axis, is_real, is_forward, inv_norm)\u001b[0m\n\u001b[1;32m     67\u001b[0m         a \u001b[38;5;241m=\u001b[39m z\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m a\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 70\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mpfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     a \u001b[38;5;241m=\u001b[39m swapaxes(a, axis, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply to all files\n",
    "midi_dir = \"../music_structure_analysis/POP909-Dataset-midi/POP909/\"\n",
    "analysis_dir = \"../music_structure_analysis/hierarchical-structure-analysis/POP909/\"\n",
    "for i in range(500, 909):\n",
    "    if i == 9: \n",
    "        continue\n",
    "    # get the index string for filename\n",
    "    idx = '{:03d}'.format(i)\n",
    "    store_recurrence_matrix(idx,midi_dir=midi_dir, analysis_dir=analysis_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
